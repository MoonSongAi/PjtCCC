# streamlit_app.py
import os

import streamlit as st
from analysis_image import load_to_image, save_image_to_folder
from langchain.memory import StreamlitChatMessageHistory

# from streamlit_chat import message
from langchain_community.callbacks import get_openai_callback
from langchain_integration import setup_langchain
from PIL import Image
from streamlit_cropper import st_cropper


def main():
    st.set_page_config(page_title="í‘œì‹œ ë””ìì¸", page_icon=":volcano:")

    st.title("_í‘œì‹œ ë””ìì¸ ì˜¤ë¥˜....?_ :red[QA Chat]_ :volcano:")

    tab1, tab2, tab3 = st.tabs(
        ["ğŸ’«Image processing....", "ğŸ§‘â€ğŸš€chat.....", "ğŸ•µï¸â€â™‚ï¸ chucked Data"]
    )

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None
    # 1ë‹¨ê³„: ì´ˆê¸° ìƒíƒœ ì„¤ì •
    if "canvas_image_data" not in st.session_state:
        st.session_state.canvas_image_data = None
    # íšŒì „ëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  session_state ì´ˆê¸°í™”
    if "rotation_angle" not in st.session_state:
        st.session_state.rotation_angle = 0
    if "saved_images" not in st.session_state:
        st.session_state.saved_images = []
    if "images_list" not in st.session_state:
        st.session_state.images_list = []
    if "loaded_image" not in st.session_state:
        st.session_state.loaded_image = None

    with st.sidebar:
        with st.expander("Select Image", expanded=True):
            uploaded_Image = st.file_uploader(
                "Select target Image", type=["pdf", "png", "jpg"]
            )
            # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë²„íŠ¼ì˜ í™œì„±í™” ìƒíƒœ ê²°ì •
            button_enabled = uploaded_Image is not None
            process_image = st.button(
                "Analysis Design file....", disabled=not button_enabled
            )

        with st.expander("Setting for LangChain", expanded=False):
            uploaded_files = st.file_uploader(
                "Upload your file", type=["pdf", "docx"], accept_multiple_files=True
            )
            # Streamlit ì‚¬ì´ë“œë°”ì— ìŠ¬ë¼ì´ë” ì¶”ê°€
            openai_api_key = st.text_input(
                "OpenAI API Key", key="chatbot_api_key", type="password"
            )
            chunk_size = st.slider(
                "Chunk Size", min_value=100, max_value=2000, value=900, step=50
            )
            chunk_overlap = st.slider(
                "Chunk Overlap", min_value=50, max_value=500, value=100, step=10
            )
            # Streamlit ì‚¬ì´ë“œë°” ì½¤ë³´ë°•ìŠ¤ ì¶”ê°€
            device_option = st.selectbox(
                "Choose the device for the model",
                options=[
                    "cpu",
                    "cuda",
                    "cuda:0",
                    "cuda:1",
                ],  # ì—¬ê¸°ì— í•„ìš”í•œ ëª¨ë“  ì˜µì…˜ì„ ì¶”ê°€í•˜ì„¸ìš”.
                index=0,  # 'cpu'ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            )
            # Streamlit ì‚¬ì´ë“œë°”  ì½¤ë³´ë°•ìŠ¤ ì¶”ê°€
            model_name = st.selectbox(
                "Choose the model for OpenAI LLM API",
                options=[
                    "gpt-3.5-turbo",
                    "gpt-3",
                    "gpt-4",
                    "davinci-codex",
                    "curie",
                ],  # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì´ë¦„ë“¤
                index=0,  # 'gpt-3.5-turbo'ë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            )
            # íŒŒì¼ì´ ì—…ë¡œë“œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë²„íŠ¼ì˜ í™œì„±í™” ìƒíƒœ ê²°ì •
            button_enabled = uploaded_files is not None and len(uploaded_files) > 0
            process_lang = st.button("Process....", disabled=not button_enabled)
    if uploaded_Image:
        if st.session_state.loaded_image != uploaded_Image:  # imageê°€ ë³€ê²½ë˜ì—ˆë‹¤ë©´
            st.session_state.loaded_image = uploaded_Image
            st.session_state.saved_images = []
            st.session_state.images_list = []
        with tab1:

            img = load_to_image(uploaded_Image)

            cropped_img = st_cropper(
                img, realtime_update=True, box_color="#0000FF", aspect_ratio=(1, 1)
            )

            # Manipulate cropped image at will
            st.session_state.canvas_image_data = cropped_img
            # _ = cropped_img.thumbnail((300,300))

            # ë²„íŠ¼ ë°°ì¹˜ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ìƒì„±
            col1, col2 = st.columns(2)
            with col1:
                # ì´ë¯¸ì§€ ì €ì¥ ë²„íŠ¼
                save_image = st.button("Save cropped image")
            with col2:
                # ì´ë¯¸ì§€ íšŒì „ ë²„íŠ¼
                rotate_image = st.button("Rotate")
            if rotate_image:
                st.session_state.rotation_angle += 90  # íšŒì „ ê°ë„ ì—…ë°ì´íŠ¸
                st.session_state.rotation_angle %= 360  # 360ë„ê°€ ë˜ë©´ 0ìœ¼ë¡œ ë¦¬ì…‹
                # í˜„ì¬ íšŒì „ ê°ë„ì— ë”°ë¼ ì´ë¯¸ì§€ íšŒì „

            st.session_state.canvas_image_data = (
                st.session_state.canvas_image_data.rotate(
                    st.session_state.rotation_angle, expand=True
                )
            )

            st.write("Cropped Image Preview")
            st.image(st.session_state.canvas_image_data)
            if save_image:
                save_name = save_image_to_folder(st.session_state.canvas_image_data)
                # ì €ì¥ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ì— ì´ë¯¸ì§€ ì¶”ê°€
                st.session_state.saved_images.append(st.session_state.canvas_image_data)
                st.session_state.images_list.append(save_name)
                st.session_state.rotation_angle = 0
            # ì €ì¥ëœ ì´ë¯¸ì§€ ì¸ë„¤ì¼ì„ íš¡ìœ¼ë¡œ ë‚˜ì—´í•˜ì—¬ í‘œì‹œ
            if st.session_state.saved_images:
                # ê° ì´ë¯¸ì§€ë¥¼ ì‘ì€ ì¸ë„¤ì¼ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
                cols = st.columns(len(st.session_state.saved_images))
                for idx, saved_image in enumerate(st.session_state.saved_images):
                    with cols[idx]:
                        # ì¸ë„¤ì¼ í¬ê¸°ë¡œ ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
                        st.caption(st.session_state.images_list[idx])
                        saved_image.thumbnail((200, 200))
                        st.image(saved_image, width=100)  # ì¸ë„¤ì¼ ì´ë¯¸ì§€ í‘œì‹œ

    if process_lang:
        if not openai_api_key:
            openai_api_key = st.secrets["OpenAI_Key"]
            if not openai_api_key:
                st.info("Please add your OpenAI API key to continue.")
                st.stop()
        # Langchain ì„¤ì •
        conversation_chain = setup_langchain(
            st,
            tab3,
            uploaded_files,
            chunk_size,
            chunk_overlap,
            device_option,
            openai_api_key,
            model_name,
        )

        st.session_state.conversation = conversation_chain

        st.session_state.processComplete = True

    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {
                "role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”! ì£¼ì–´ì§„ ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹  ê²ƒì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš”!",
            }
        ]

    for message in st.session_state.messages:
        with tab2.chat_message(message["role"]):
            tab2.markdown(message["content"])

    history = StreamlitChatMessageHistory(key="chat_messages")

    # Chat logic
    query = tab2.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    if query:
        st.session_state.messages.append({"role": "user", "content": query})

        with tab2.chat_message("user"):
            tab2.markdown(query)

        with tab2.chat_message("assistant"):
            chain = st.session_state.conversation

            with st.spinner("Thinking..."):
                result = chain({"question": query})
                with get_openai_callback() as cb:
                    st.session_state.chat_history = result["chat_history"]
                response = result["answer"]
                source_documents = result["source_documents"]

                tab2.markdown(response)
                with tab2.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    tab2.markdown(
                        source_documents[0].metadata["source"],
                        help=source_documents[0].page_content,
                    )
                    tab2.markdown(
                        source_documents[1].metadata["source"],
                        help=source_documents[1].page_content,
                    )
                    tab2.markdown(
                        source_documents[2].metadata["source"],
                        help=source_documents[2].page_content,
                    )


if __name__ == "__main__":
    main()
